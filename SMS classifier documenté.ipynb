{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import streamlit as st\n",
    "\n",
    "# Télécharger les ressources nécessaires pour NLTK\n",
    "nltk.download('stopwords')  # Télécharge les mots vides pour la langue anglaise\n",
    "nltk.download('wordnet')  # Télécharge le dictionnaire WordNet pour la lemmatisation\n",
    "\n",
    "# Constantes\n",
    "OUTPUT_DIR = \"spam_classifier_output\"  # Dossier pour sauvegarder les fichiers de sortie\n",
    "MODEL_PATH = os.path.join(OUTPUT_DIR, \"spam_classifier_model.pkl\")  # Chemin du fichier modèle\n",
    "VECTORIZER_PATH = os.path.join(OUTPUT_DIR, \"tfidf_vectorizer.pkl\")  # Chemin du fichier vectoriseur\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)  # Crée le dossier de sortie s'il n'existe pas\n",
    "\n",
    "# Nettoyage des données\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte pour la classification.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Retourne une chaîne vide si l'entrée n'est pas une chaîne\n",
    "    text = text.lower()  # Convertit le texte en minuscule\n",
    "    text = re.sub(r'http\\S+|www\\S+', 'url', text)  # Remplace les URLs par 'url'\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Supprime les caractères non alphabétiques\n",
    "    stop_words = set(stopwords.words('english'))  # Liste des mots vides en anglais\n",
    "    lemmatizer = WordNetLemmatizer()  # Initialisation du lemmatiseur\n",
    "    return ' '.join(\n",
    "        lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words\n",
    "    )  # Lemmatisation et suppression des mots vides\n",
    "\n",
    "# Charger ou sauvegarder le modèle et le vectoriseur\n",
    "def load_model_and_vectorizer():\n",
    "    \"\"\"Charge le modèle et le vectoriseur à partir des fichiers.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(MODEL_PATH) and os.path.exists(VECTORIZER_PATH):\n",
    "            model = joblib.load(MODEL_PATH)  # Charge le modèle sauvegardé\n",
    "            vectorizer = joblib.load(VECTORIZER_PATH)  # Charge le vectoriseur sauvegardé\n",
    "            return model, vectorizer\n",
    "    except Exception as e:\n",
    "        st.error(f\"Erreur lors du chargement : {e}\")  # Affiche une erreur dans l'interface Streamlit\n",
    "    return None, None  # Retourne None si le chargement échoue\n",
    "\n",
    "def save_model_and_vectorizer(model, vectorizer):\n",
    "    \"\"\"Sauvegarde le modèle et le vectoriseur.\"\"\"\n",
    "    joblib.dump(model, MODEL_PATH)  # Sauvegarde le modèle\n",
    "    joblib.dump(vectorizer, VECTORIZER_PATH)  # Sauvegarde le vectoriseur\n",
    "\n",
    "# Entraîner le modèle\n",
    "def train_model(data: pd.DataFrame):\n",
    "    \"\"\"Entraîne un modèle Random Forest avec un TfidfVectorizer.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)  # Initialise le vectoriseur TF-IDF\n",
    "    model = RandomForestClassifier(random_state=42, class_weight='balanced')  # Initialise le modèle avec des poids équilibrés\n",
    "    messages = data['message'].astype(str).apply(clean_text)  # Nettoie les messages\n",
    "    labels = data['label'].astype(int)  # Convertit les étiquettes en entiers\n",
    "    X = vectorizer.fit_transform(messages)  # Transforme les messages en vecteurs TF-IDF\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)  # Divise les données en ensembles d'entraînement et de test\n",
    "\n",
    "    # Calculer les poids des échantillons\n",
    "    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)  # Calcule les poids pour équilibrer les classes\n",
    "\n",
    "    # Optimisation des hyperparamètres\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],  # Nombre d'arbres dans la forêt\n",
    "        'max_depth': [5, 10, 15],  # Profondeur maximale des arbres\n",
    "    }\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1', n_jobs=-1)  # Recherche sur grille avec validation croisée\n",
    "    grid_search.fit(X_train, y_train)  # Entraîne le modèle avec les hyperparamètres optimaux\n",
    "\n",
    "    # Extraire les résultats du GridSearch\n",
    "    results = pd.DataFrame(grid_search.cv_results_)  # Convertit les résultats en DataFrame\n",
    "\n",
    "    # Visualisation des résultats du GridSearch\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=results['param_n_estimators'], y=results['mean_test_score'], label='Mean F1 Score')  # Trace la courbe des scores\n",
    "    plt.title('Résultats de la Recherche de Grille')\n",
    "    plt.xlabel(\"Nombre d'Estimateurs\")\n",
    "    plt.ylabel('Score F1 Moyen')\n",
    "    plt.legend()\n",
    "    grid_search_path = os.path.join(OUTPUT_DIR, 'grid_search_results.png')  # Chemin pour sauvegarder la figure\n",
    "    plt.savefig(grid_search_path)\n",
    "    plt.close()\n",
    "\n",
    "    model = grid_search.best_estimator_  # Récupère le meilleur modèle\n",
    "    model.fit(X_train, y_train)  # Entraîne le meilleur modèle\n",
    "    save_model_and_vectorizer(model, vectorizer)  # Sauvegarde le modèle et le vectoriseur\n",
    "\n",
    "    # Évaluation sur les données de test\n",
    "    y_pred = model.predict(X_test)  # Prédictions sur l'ensemble de test\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]  # Probabilités prédictives pour la classe positive\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)  # Calcule la courbe ROC\n",
    "    roc_auc = auc(fpr, tpr)  # Calcule l'aire sous la courbe ROC\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Courbe ROC (aire = {roc_auc:.2f})')  # Trace la courbe ROC\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Trace la diagonale\n",
    "    plt.xlabel('Taux de Faux Positifs')\n",
    "    plt.ylabel('Taux de Vrais Positifs')\n",
    "    plt.title('Courbe ROC (Receiver Operating Characteristic)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    roc_path = os.path.join(OUTPUT_DIR, 'roc_curve.png')  # Chemin pour sauvegarder la figure\n",
    "    plt.savefig(roc_path)\n",
    "    plt.close()\n",
    "\n",
    "    st.subheader(\"Évaluation sur les données de test\")\n",
    "    st.text(classification_report(y_test, y_pred))  # Affiche le rapport de classification\n",
    "    st.image(roc_path, caption=\"Courbe ROC\")  # Affiche la courbe ROC\n",
    "    st.image(grid_search_path, caption=\"Résultats de la Recherche de Grille\")  # Affiche les résultats du GridSearch\n",
    "\n",
    "    return model, vectorizer\n",
    "\n",
    "# Générer des métriques\n",
    "def generate_metrics(data: pd.DataFrame, model, vectorizer):\n",
    "    \"\"\"Génère les métriques de performance et une matrice de confusion.\"\"\"\n",
    "    messages = data['message'].astype(str).apply(clean_text)  # Nettoie les messages\n",
    "    X = vectorizer.transform(messages)  # Transforme les messages en vecteurs TF-IDF\n",
    "    y_true = data['label'].astype(int)  # Étiquettes réelles\n",
    "    y_pred = model.predict(X)  # Prédictions du modèle\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)  # Génère le rapport de classification\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)  # Génère la matrice de confusion\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Spam', 'Spam'], yticklabels=['Non-Spam', 'Spam'])  # Trace la matrice de confusion\n",
    "    plt.title('Matrice de Confusion (Confusion Matrix)')\n",
    "    plt.xlabel('Prédit')\n",
    "    plt.ylabel('Réel')\n",
    "    confusion_path = os.path.join(OUTPUT_DIR, 'confusion_matrix.png')  # Chemin pour sauvegarder la figure\n",
    "    plt.savefig(confusion_path)\n",
    "    plt.close()\n",
    "\n",
    "    return report, confusion_path\n",
    "\n",
    "# Interface utilisateur Streamlit\n",
    "st.title(\"Analyseur SMS Spam - Classificateur avec Visualisation\")  # Titre de l'application\n",
    "\n",
    "# Réentraîner le modèle\n",
    "st.header(\"Réentraîner le Modèle\")  # En-tête pour la section de réentraînement\n",
    "uploaded_file = st.file_uploader(\"Choisissez un fichier CSV\", type=[\"csv\"])  # Téléchargement d'un fichier CSV\n",
    "if uploaded_file:\n",
    "    try:\n",
    "        data = pd.read_csv(uploaded_file, delimiter='\\t', header=None, names=['label', 'message'])  # Charge les données\n",
    "        data['label'] = data['label'].map({'ham': 0, 'spam': 1})  # Mappe les étiquettes de texte à des valeurs numériques\n",
    "        model, vectorizer = train_model(data)  # Entraîne le modèle\n",
    "        st.success(\"Modèle réentraîné avec succès.\")  # Affiche un message de succès\n",
    "\n",
    "        report, confusion_path = generate_metrics(data, model, vectorizer)  # Génère les métriques\n",
    "        st.subheader(\"Matrice de Confusion\")\n",
    "        st.image(confusion_path)  # Affiche la matrice de confusion\n",
    "        st.subheader(\"Métriques de Performance\")\n",
    "        st.write(pd.DataFrame(report).transpose())  # Affiche les métriques sous forme de tableau\n",
    "    except Exception as e:\n",
    "        st.error(f\"Erreur : {e}\")  # Affiche une erreur en cas de problème\n",
    "\n",
    "# Charger modèle et vectoriseur\n",
    "model, vectorizer = load_model_and_vectorizer()  # Charge le modèle et le vectoriseur\n",
    "\n",
    "# Section de prédiction\n",
    "st.header(\"Prédire un SMS\")  # En-tête pour la section de prédiction\n",
    "user_input = st.text_area(\"Entrez un SMS à analyser :\", height=100)  # Champ de saisie pour le texte à prédire\n",
    "\n",
    "if st.button(\"Classer le SMS\"):\n",
    "    if not model or not vectorizer:\n",
    "        st.warning(\"Modèle non disponible. Veuillez réentraîner le modèle.\")  # Avertissement si le modèle est manquant\n",
    "    else:\n",
    "        input_vectorized = vectorizer.transform([clean_text(user_input)])  # Transforme le texte utilisateur en vecteur TF-IDF\n",
    "        prediction = model.predict(input_vectorized)[0]  # Prédit la classe du SMS\n",
    "        st.success(f\"Ce SMS est classé comme : **{'Spam' if prediction == 1 else 'Ham (Non-Spam)'}**\")  # Affiche le résultat de la prédiction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
